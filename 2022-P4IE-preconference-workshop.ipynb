{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uuATYYtzkB2"
      },
      "source": [
        "<font size='10' color = 'E3A440'>**Pre-conference Workshop on Python**</font>\n",
        "=======\n",
        "<font color = 'E3A440'>*Pratical introduction to the analysis of unstructured data*</font>\n",
        "=============\n",
        "\n",
        "This tutorial is a short hands-on workshop to introduce the analysis of unstructured data for innovation studies. \n",
        "\n",
        "Structure of the workshop:\n",
        "1. Presentation of sections 1 and 2 in a plenary mode (20 minutes)\n",
        "2. Individual work on section 3 (20 minutes)\n",
        "3. Group work on section 4 (60 minutes)\n",
        "4. Plenary session with groups presentations (20 minutes)\n",
        "\n",
        "This tutorial cannot be considered exhaustive of the domain of textual data analysis. \n",
        "\n",
        "### Authors: \n",
        "- Mikaël Héroux-Vaillancourt <mikael.heroux-vaillancourt@polymtl.ca>\n",
        "- Davide Pulizzotto <davide.pulizzotto@polymtl.ca>\n",
        "- Pietro Cruciata <pietro.cruciata@polymtl.ca>\n",
        "- Nicolas Sacchetti <nicolas.sacchetti@polymtl.ca>\n",
        "- Geneviève Ratelle <genevieve.ratelle@gmail.com> \n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Section 0. Introduction](#introduction)\n",
        "- [Section 1. Data Pre-Processing](#pre-processing)\n",
        "- [Section 2. Descriptive statistics](#desc-stat)\n",
        "- [Section 3. Analysis](#analysis)\n",
        "- [Section 4. Exercise](#exercie)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "232GZfpEzkB5"
      },
      "source": [
        "<a id='introduction'></a>\n",
        "# <font size = '6' color='E3A440'>Section 0. Introduction</font>\n",
        "\n",
        "This workshop is based on the research work of **Mikaël Héroux-Vaillancourt** presented at the [P4IE Conference - Measuring Metrics that Matter](https://event.fourwaves.com/p4ie/pages), which will take place  on 9-10-11 May 2022 at the *Hilton Garden Inn*, in Ottawa.\n",
        "\n",
        "Here the [link]() to get the presentation of Mikaël Héroux-Vaillancourt.\n",
        "\n",
        "In order to succeed in this workshop, a few concepts have to be underlined:\n",
        "\n",
        "1. This study is an exploratory analysis of several companies which obtained a [BCorp certification](https://www.bcorporation.net/en-us/).\n",
        "2. The textual data analyzed come from a web scraping step of the snapshots found on [Wayback Machine](https://archive.org/web/) of all those companies of point 1.\n",
        "3. The main objective is to group the B Corp companies by similarities between their key activities.\n",
        "\n",
        "### Abstract of the Study\n",
        "In this exploratory study, we used the web content of 1,110 enterprises audited and certified as B Corp to better understand the context that would lead organizations to undertake this process. After scraping and cleaning the data, we processed it in order to  generate clusters using the k-means algorithm. Our results show 20 clusters where we can easily see key activities, sectors and B Corp signalling. This is intended as an introduction to NLP tools to hopefully be used as an inspiration for your own research.\n",
        "\n",
        "### Glossary of the workshop\n",
        "\n",
        "Below is a list of some terms used in this workshop:\n",
        "\n",
        "1. <font color='E3A440'><b>Dataframe</b></font>: a data structure that organizes data into a 2-dimensional table of rows and columns\n",
        "2. <font color='E3A440'><b>Metadata</b></font>: it is information about an unstructured block of data (i.e., the author of a paper is metadata of the text of that paper).\n",
        "3. <font color='E3A440'><b>Token</b></font>: each sequence of character which constitutes an independent linguistic unit, that is a word (i.e., each occurrence of the verb eat is a different token).\n",
        "4. <font color='E3A440'><b>Type</b></font>: The unique string of character which represents several occurrences of the same linguistic unit or word (i.e., the string 'eat' is the type for each token of the verb to eat).\n",
        "5. <font color='E3A440'><b>POS tag</b></font>:  a process which aims to assign parts of speech to each word of a given text\n",
        "6. <font color='E3A440'><b>Stopwords</b></font>: it refers to words that are very low semantic content such as articles (the, an), modals (would, must), etc. \n",
        "7. <font color='E3A440'><b>Lemmatization</b></font>:  in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form\n",
        "8. <font color='E3A440'><b>Document-Term Matrix (DTM) and vectorization</b></font>: the DTM is a matrix where rows represent segments of text and columns are filled by linguistic features of those segments of text. Vectorization is the process that transforms raw texts in a matrix.\n",
        "9. <font color='E3A440'><b>Weighting</b></font>: it is the weight a mathematical function provides for each linguistic feature of each segment of text (i.e., Term frequency, Tf-Idf, BM25, etc.)\n",
        "10. <font color='E3A440'><b>K-means clustering</b></font>: an unsupervised statistic method aiming to partition *n* observations into *k* clusters based on their proximity to the centroid.\n",
        "\n",
        "\n",
        "### Some Basic Concepts\n",
        "The main step in text mining is to convert unstructured textual data into a mathematical model to be used in statistical learning. Thus, we need to create a <font color='E3A440'>**Document-Term matrix**</font>, a matrix $n \\times w$, where $n$ is the number of text segments and $w$ is the number of textual features selected.The textual feature can have different nature. In the simplest model, these features correspond to the set of types that resume each token of the corpus. In other terms, $w$ is the number of features that characterize a segment of text. The matrix is generally represented as follows:  \n",
        " \n",
        "$$X = \\begin{bmatrix} \n",
        "x_{11} & x_{12} & \\ldots & x_{1w} \\\\\n",
        "\\vdots & \\vdots       &  \\ddots      & \\vdots \\\\ \n",
        "x_{n1} & x_{12} & \\ldots & x_{nw} \\\\\n",
        "\\end{bmatrix}\n",
        "$$ \n",
        " \n",
        "\\\\\n",
        "When we apply a <font color='E3A440'>**clustering algorithm**</font> to this matrix, we want to group rows in a homogeneous set of clusters. This means to minimize the intra-class inertia or the pairwise squared deviations of points in the same cluster:\n",
        "\n",
        "$$ \\underset{s}{\\arg\\min}\\sum_{i=1}^{k}\\frac{1}{\\vert S_i\\vert}\\sum_\\limits{x,y \\in S_i}\\Vert x-y \\Vert^2$$\n",
        "\n",
        "This kind of algorithms aims to create a vector $Y$ of size $n$, containing the <font color='E3A440'>**cluster label**</font> assigned to each segment of text from $1$ to $k$.\n",
        "\n",
        "$$\n",
        "Y = \\begin{bmatrix} \n",
        "c_1 \\\\\n",
        "c_2 \\\\\n",
        "\\vdots \\\\ \n",
        "c_n\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Thus, $Y_1$ corresponds to the cluster label given at $X_1$. Generally, $k$ is the main parameters of a clustering algorithm which represents the number of cluster into which group text segments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRKQ8i7BzkB7"
      },
      "source": [
        "## 0.1 Preparation of Colab Virtual Machine\n",
        "\n",
        "In order to work correctly on Colab, we need to prepare the environment by executing two main steps:\n",
        "1. Download data from the GitHub project \n",
        "2. Install packages to run code of this workshop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9mMQQWJzkB7"
      },
      "outputs": [],
      "source": [
        "# Downloading of data from the GitHub project\n",
        "!rm -rf TEST_PRECONFERENCE/\n",
        "!git clone https://github.com/puli83/TEST_PRECONFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation of packages\n",
        "!pip install pickle5\n",
        "!pip install nltk\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "GHzhKocKltaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmzFMxo-zkB8"
      },
      "source": [
        "## 0.2 Import packages\n",
        "\n",
        "We import the following packages that contain the functions needed to accomplish our task: \n",
        "- `re` package for REGEX\n",
        "- `matplotlib` package for plotting\n",
        "- `numpy` package for mathematical operations and data optimization\n",
        "- `pandas` package for the manipulation of data \n",
        "- `sklearn` (scikit-learn) package for machine learning modelling\n",
        "- `spacy` package for Natural Language Processing, containing pre-trained models\n",
        "- `nltk` package for Natural Language Processing, including over 50 corpora, and several other resources "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import datetime\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import pickle5 as pickle\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "tlI8PkH18Ofi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 Definition of custom functions\n",
        "In the next chunk of code, we define the functions that we need to reach the goal of the workshop. These functions will not be explained in detail since their comprehension is not essential to complete the workshop."
      ],
      "metadata": {
        "id": "KNlCfnZ79Ec5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_for_WC(DTM, vocabulary_dtm):\n",
        "    # compute total frequency for each word\n",
        "    values_words = sum(DTM)\n",
        "    # values_words = sum(tfidf_matrix)\n",
        "    # verify type result and prepare data for wordcloud\n",
        "    if type(values_words) is np.ndarray:\n",
        "        values_words = [float(value) for value in np.nditer(values_words)]\n",
        "    elif type(values_words) is scipy.sparse.csr.csr_matrix:\n",
        "        values_words = [float(value) for value in np.nditer(values_words.todense())]\n",
        "    else:\n",
        "        print(\"Matrix in argument DTM has to be one of these two data classes:  'scipy.sparse.csr.csr_matrix' or 'numpy.ndarray'\")\n",
        "    ##Retrieve the word fromthe vocaboulary and sorting them based on the frequency\n",
        "    list_mots = sorted(vocabulary_dtm.items(), key= lambda x:x[1])\n",
        "    list_mots = [word for (word,idx) in  list_mots]\n",
        "    words = zip(list_mots, values_words)\n",
        "    words = sorted(words, key= lambda x:x[1], reverse=True)\n",
        "    ## prepare data structure for wordcloud\n",
        "    result_for_WC = {}\n",
        "    #iterating over the tuples lists\n",
        "    for (key, value) in words:\n",
        "        result_for_WC[key] = value\n",
        "    #\n",
        "    return result_for_WC\n",
        "\n",
        "def lexical_keyness(DTM, cls_kmeans, vocabulary_vectorize, n_cluster = 0):\n",
        "    import math\n",
        "    # This code ref takes inspiration from this python module : https://pypi.org/project/corpus-toolkit/\n",
        "    # and its main script:  https://github.com/kristopherkyle/corpus_toolkit/blob/master/corpus_toolkit/corpus_tools.py\n",
        "    # which is based on this paper: https://aclanthology.org/J93-1003/\n",
        "\n",
        "    cluster_keyness = n_cluster\n",
        "    df_freq_target = pd.DataFrame(np.asarray(DTM[cls_kmeans.labels_ == cluster_keyness].sum(0).T).reshape(-1))#, columns = [word for (word,idx) in sorted(vectorized.vocabulary_.items(), key= lambda x:x[1])]))\n",
        "    df_freq_target.index = [word for (word,idx) in sorted(vocabulary_vectorize.items(), key= lambda x:x[1])]\n",
        "    df_freq_target.index\n",
        "    df_freq_target.columns = ['freq1']\n",
        "    df_freq_target['freq2'] = np.asarray(DTM[~(cls_kmeans.labels_ == cluster_keyness)].sum(0).T).reshape(-1)\n",
        "    #\n",
        "    df_freq_target['tot'] = df_freq_target['freq1'] + df_freq_target['freq2']\n",
        "    #\n",
        "    df_freq_target['freq1'] = df_freq_target['freq1'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    df_freq_target['freq2'] = df_freq_target['freq2'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    #\n",
        "    df_freq_target['freq1_norm'] = df_freq_target['freq1']/df_freq_target['freq1'].sum() * 1000000\n",
        "    df_freq_target['freq2_norm'] = df_freq_target['freq2']/df_freq_target['freq2'].sum() * 1000000\n",
        "    #\n",
        "    df_freq_target['fraction'] = df_freq_target['freq1_norm'] / df_freq_target['freq2_norm']\n",
        "    df_freq_target['Log-likelihood Ratio'] = df_freq_target['fraction'].apply(math.log2)\n",
        "    return df_freq_target\n",
        "\n",
        "def Clustering_kmeans(DTM, reduce_lsa = False, n_comp_lsa= 300, n_clusters = 2,  **kwargs):\n",
        "    \"\"\"\n",
        "    Paramaters:\n",
        "    DTM;  Matrice to pass under sklearn classifiers\n",
        "    n_clusters = 2; the number of cluster to generate.\n",
        "    reduce_lsa = False; If True, a SVD is executed on the DTM before to execute the clustering\n",
        "    n_comp_lsa= 300; Number of component to keep after SVD on DTM.\n",
        "    **kwargs; Arguments for Kmeans method of sklearn.cluster. For exemple :\n",
        "    random_state = 1234,\n",
        "    max_iter = 300,\n",
        "    n_init = 20,\n",
        "    tol = .000001,\n",
        "    init = \"k-means++\",\n",
        "    n_jobs = -1,\n",
        "    precomputeDistances = True\n",
        "    \"\"\"\n",
        "    import datetime\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn import metrics\n",
        "    from sklearn.pipeline import make_pipeline\n",
        "    from sklearn.preprocessing import Normalizer\n",
        "    \n",
        "    ## dimensionality reduction\n",
        "    if reduce_lsa == True:\n",
        "        from sklearn.decomposition import TruncatedSVD\n",
        "        svd = TruncatedSVD(n_comp_lsa,\n",
        "                           n_iter = 100,\n",
        "                           random_state = kwargs['random_state'])\n",
        "        #\n",
        "        X_lsa = svd.fit_transform(DTM)\n",
        "        explained_variance = svd.explained_variance_ratio_.sum()\n",
        "        print(\"Explained variance of the SVD step: {}%\".format(\n",
        "            int(explained_variance * 100)))\n",
        "        #\n",
        "        DTM = X_lsa\n",
        "    # Normalize samples individually to unit norm. Scaling inputs to unit norms is a common operation for text classification or clustering for instance.\n",
        "    DTM = Normalizer().transform(DTM)\n",
        "    cls_kmeans = KMeans(n_clusters = n_clusters,\n",
        "                        random_state = kwargs['random_state'],\n",
        "                        max_iter = kwargs['max_iter'],\n",
        "                        n_init = kwargs['n_init'],\n",
        "                        tol = kwargs['tol'],\n",
        "                        init = kwargs['init'])\n",
        "    \n",
        "    cluster_labels = cls_kmeans.fit_predict(DTM)\n",
        "    ## silhouette\n",
        "    silhouette_avg = metrics.silhouette_score(DTM, cluster_labels)\n",
        "    print(\"For n_clusters = %i;\\t The silhouette_avg is : %f5\" %(n_clusters, silhouette_avg))\n",
        "    #\n",
        "    cls_kmeans.DTM_ = DTM\n",
        "    return cls_kmeans\n",
        "\n",
        "def wordcloud_par_cluster(wordcloud, DTM, cls_kmeans, vocab, first_n_words=10, figsize=(18, 16), fontsize=32, plot_wordcloud = True, lst_clust = [], title_in_plot = \"Clust_\"):\n",
        "\n",
        "        \"\"\"\n",
        "        wordcloud; A WordCloud function.\n",
        "        DTM; A Docuemnt-Term Matrix\n",
        "        vocab; It is a vocabulary from skllarn vectorizer\n",
        "        first_n_words = 10; How many words to print\n",
        "        figsize = (18, 16); Size of the plot. (this is the argument of this line plt.figure(figsize=figsize))\n",
        "        fontsize = 32; Size of title font\n",
        "        lst_clust = []; The list of cluster to plot. If empty, all the clusters are plotted\n",
        "        title_in_plot = \"Clust_\"; title to put on top of plot \\n\n",
        "        \"\"\"\n",
        "        import numpy\n",
        "        import scipy\n",
        "        \n",
        "        if not lst_clust:\n",
        "            lst_clust = list(range(cls_kmeans.n_clusters))\n",
        "\n",
        "        for x in lst_clust:\n",
        "            DTM_temp = DTM[cls_kmeans.labels_ == x]\n",
        "            result_for_WC= prepare_data_for_WC(DTM_temp, vocab)\n",
        "            ###\n",
        "            if plot_wordcloud == True:\n",
        "                plot = wordcloud.generate_from_frequencies(result_for_WC)\n",
        "                plt.figure(figsize=figsize)\n",
        "                plt.imshow(plot)\n",
        "                plt.title(title_in_plot + str(x) + '  N. of documents=' + str(DTM_temp.shape[0]),\n",
        "                        fontsize = fontsize,\n",
        "                        bbox=dict(facecolor='red', alpha=0.5))\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "            print(f\"Most frequent words for cluster {x} of size {str(DTM_temp.shape[0])} docs: \", list(result_for_WC)[0:first_n_words])\n",
        "\n",
        "def plot_data_by_cluster(DTM, cls_kmeans, figsize = (16,10) ):\n",
        "    ## Reduction of dimension to 2 for visualisation reasons\n",
        "    from sklearn.manifold import TSNE\n",
        "    time_start = time.time()\n",
        "    tsne = TSNE(n_components=2, verbose=1, perplexity=30, n_iter=1000,metric='cosine', learning_rate=10, random_state = 794)\n",
        "    reduc_dim_results = tsne.fit_transform(DTM)\n",
        "    print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "    ## Create data structure for plotting\n",
        "    df_reduction = pd.DataFrame()\n",
        "    df_reduction['y'] =  cls_kmeans.labels_\n",
        "    df_reduction['1-dim'] = reduc_dim_results[:,0]\n",
        "    df_reduction['2-dim'] = reduc_dim_results[:,1]\n",
        "\n",
        "    ## Generate the plot\n",
        "    import seaborn as sns\n",
        "    import colorcet as cc\n",
        "    plt.figure(figsize = figsize)\n",
        "    sns.scatterplot(data = df_reduction,\n",
        "                    x=\"1-dim\",\n",
        "                    y=\"2-dim\",\n",
        "                    hue=\"y\",\n",
        "                    palette = sns.color_palette(cc.glasbey, n_colors = cls_kmeans.n_clusters),)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def print_KWIC_clusters(data, DTM, cls_kmeans, vocab, k_to_analyze = 0, n_words = 3, n_segms_per_docuemnt = 5, n_documents = 3, left_windows = 50, right_windows = 50):\n",
        "    DTM_temp = DTM[cls_kmeans.labels_ == k_to_analyze]\n",
        "    result_for_WC= prepare_data_for_WC(DTM_temp, vocab)\n",
        "    words = list(result_for_WC)[0:n_words]\n",
        "    print(f\"The following words are used for the Keyword In Context extraction: {words}\")\n",
        "    df_temp = data[data[\"cluster_labels\"] == k_to_analyze].sort_values(by='Cosine_of_related_cluster', ascending= False).iloc[0:n_documents]\n",
        "    for idx, row in df_temp.iterrows():\n",
        "        n_company = row['company_name']\n",
        "        for x in [f\"Company : {n_company}.\" + f\" Segment number: {str(y)}.\" + ' Segment: ' + '**'.join(x) for y,x in enumerate(re.findall(\"(.{0,\" + str(left_windows) +\"})(\" + \"|\".join(words)+ \")(.{0,\"+ str(right_windows) + \"})\", row[\"text_web_page\"]))][0:n_segms_per_docuemnt]:\n",
        "            print(x)"
      ],
      "metadata": {
        "id": "ORZsrIP9lm7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.4 Import data\n",
        "\n"
      ],
      "metadata": {
        "id": "ebi993i_8XPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code sets the path from which to import data."
      ],
      "metadata": {
        "id": "CGayi-cExJSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR='TEST_PRECONFERENCE/'\n",
        "DATA_DIR=os.path.join(ROOT_DIR, 'Data')"
      ],
      "metadata": {
        "id": "E6Th_5mB8rAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "We need to import the `.csv` file with the company’s data. The data is imported in a tabular format (such as MS Excel) called **dataframe** and managed by `pandas`. We named the variable containing data as follows `df_data_bcorp`"
      ],
      "metadata": {
        "id": "Xsjk2gvXOmol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_bcorp = pd.read_csv(os.path.join(DATA_DIR, 'BCorp_companies_Web_Data.csv'))"
      ],
      "metadata": {
        "id": "OS5_Fn13MTMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The dataset has *1110* rows and *129* columns."
      ],
      "metadata": {
        "id": "SmzMy4Lf68rA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_bcorp.shape"
      ],
      "metadata": {
        "id": "fxp4AwWh7Ffv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Further exploring some rows, we focus on two columns: \n",
        "\n",
        "+ **company_id** contains the domain of each BCorp companies under study\n",
        "+ **text_web_page** contains a combination of all those pages founded in Wayback Machine for each company."
      ],
      "metadata": {
        "id": "zj32jGwz7Min"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_bcorp.head()"
      ],
      "metadata": {
        "id": "_n2JxRkq7Ypn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Below, we give a list of some important metadata to explore:"
      ],
      "metadata": {
        "id": "cTlRTjB3R6Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_bcorp[['website', 'company_name', 'date_certified', 'description', 'industry','sector',\n",
        "       'country', 'state', 'city', 'assessment_year', 'overall_score']]\n"
      ],
      "metadata": {
        "id": "-a-NYE6ZSIeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font size = '6' color='E3A440'>Section 1: Data Pre-Processing </font>\n",
        "This section presents the main aspects of a classical pre-processing step of textual data. The main objective of this step is to organize unstructured data into a structure that can be digestible in a statistical learning process. In other words, this process allows transforming the text into vectors and is divided into three main operations:\n",
        " 1. Morphological Analysis\n",
        " 2. Filter of lexical features\n",
        " 3. Vectorization of lexical features"
      ],
      "metadata": {
        "id": "4CJuakgp8BhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Cleaning of metadata\n",
        "In this section, we provide a data cleaning step for our dataset. Since the state of Québec is written on two forms, one with accent and one without it, this issue generates two values for the same data.\n",
        "Thus, we replace each \"é\"  with a simple \"e\"."
      ],
      "metadata": {
        "id": "gD6_woCdorMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_bcorp['state'] = df_data_bcorp['state'].str.replace(\"é\",\"e\")\n",
        "df_data_bcorp['city'] = df_data_bcorp['city'].apply(lambda x: \"Montreal\" if re.match(\"mont\", x,  re.IGNORECASE) else x)"
      ],
      "metadata": {
        "id": "h7YZLu4goyaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Morphological Analysis\n",
        "An important operation during the pre-processing of unstructured textual data is to detect the <font color = 'E3A440'>**Morphological features**</font> of words. This allows understanding of the roles that words have in their contexts. This operation is composed of two parts:\n",
        "1. Part-of-Speech tagging, generally known as POS tagging\n",
        "2. Lemmatisation, which consists in the reduction of a word to his lemma\n",
        "\n",
        "These two sub-operations can be executed by different types of algorithms. We use a neural network pre-trained model using the module named `spacy`. In particular, we use the function `nlp.pipe()` to generate a list of preprocessed documents. This function has several arguments and it takes a list of string characters to be executed. Each element of this list is a textual segment or a document. "
      ],
      "metadata": {
        "id": "CfUdgcOHzFV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this first chunk of code, we load from `spacy` the pre-trained model  called <font color=\"#CE9178\">en_core_web_sm</font> ([More info](https://spacy.io/models/en)).   "
      ],
      "metadata": {
        "id": "GNOg27M0jUiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "hcxxLkyh-4Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In this second chunk, we apply the <font color = 'E3A440'>NLP pipeline</font> to our textual data contained in the **text_combined** column, and this, by using the `nlp.pipe()` function. \n",
        "\n",
        "Thus, we generate the `preprocessed_docs` variable, namely a list containing preprocessed text of all the documents. We can observe the first document with `preprocessed_docs[0]`, the second with `preprocessed_docs[1]`, the third with `preprocessed_docs[2]`, etc.\n",
        "\n",
        "Each element of the list `preprocessed_docs`, contains a list of words analyzed with `spacy`.\n"
      ],
      "metadata": {
        "id": "B9SKcfPxPvpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = datetime.datetime.now() # line of code to register a timestamp \n",
        "df_data_bcorp[\"text_preprocessed\"] = list(nlp.pipe(df_data_bcorp[\"text_web_page\"], disable = [\"tok2vec\",'parser','ner']))\n",
        "print(str(datetime.datetime.now() - t0)) # line of code to print the elapsed time from t0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f39aab0f-84c7-4d1f-867d-49377faf8c94",
        "id": "rVAjPpkiUgAG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:00:22.284763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the next chunk, we look at the first document in his **original format**"
      ],
      "metadata": {
        "id": "LrzGwAk0_0vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print first segment of the corpus\n",
        "print(df_data_bcorp.iloc[0][\"text_web_page\"])"
      ],
      "metadata": {
        "id": "yeiIr4ECXX82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Below, we look at the **preprocessed version** of this first document. In particular, we print three attributes for each word of the first document:\n",
        "1. `word.text`, which corresponds to the original version of the word, which is called **token**.\n",
        "2. `word.pos_`, which corresponds to the POS tag predicted for that word.\n",
        "3. `word.lemma_`, which corresponds to the lemma of the token.\n",
        "\n",
        "For pedagogical reasons, we only look at the first 10 words.\n"
      ],
      "metadata": {
        "id": "igbdW_Pc_8hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print morphosyntactic analysis of the first sentence of the corpus. Each element is separated by a vertical line |\n",
        "for idx, word in enumerate(df_data_bcorp[\"text_preprocessed\"].iloc[0]):\n",
        "    print(\"Token: \", word.text, \" | \", \"POS tag: \", word.pos_,\" | \", \"Lemma of the token: \", word.lemma_)\n",
        "    # Break loop after the first 10 words -> idx==10\n",
        "    if idx == 10:\n",
        "        break"
      ],
      "metadata": {
        "id": "pXZrHo9ETUiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Filter of lexical features\n",
        "\n",
        "In this section, we perform several pre-processing steps to transform the text to a format that can be understood and analyzed by our methods. \n",
        "1.   <font color = 'E3A440'>Eliminating stopwords</font> : removing the low-level information from our text in order to give more focus to the important information\n",
        "2.   <font color = 'E3A440'>Keeping a selected list of POS tags</font>, that is adverb, adjective, noun and verb\n"
      ],
      "metadata": {
        "id": "DW_cSk1TAayz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following chunk, we load the list of English **stopwords** from the package `nltk`. Since the list is customizable, we add the verb \"would\" that is not among the modal verbs included in this list. "
      ],
      "metadata": {
        "id": "a0Y3jeyUg3X6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download default list of stopword\n",
        "nltk.download('stopwords')\n",
        "# add a custom word to the stopword list\n",
        "stopwords_list = set(stopwords.words('english') + ['would'])"
      ],
      "metadata": {
        "id": "W7TCJHpej-RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the following chunk of code, we will use the previous list of stopwords to filter words and the POS tag analysis to select only the words that are adverbs, adjectives, nouns and verbs to further highlight the part of speech meaningful for our task."
      ],
      "metadata": {
        "id": "fiIwdtOEdfGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, row in df_data_bcorp.iterrows():\n",
        "    print(f\"Sentence n. {idx}\")\n",
        "    print(\"Original :\\t \",row['text_web_page'])\n",
        "    print(\"Preprocessed:\\t \", [w.lemma_.lower() for w in row['text_preprocessed'] if w.pos_ in [\"ADV\", \"ADJ\", \"NOUN\", \"VERB\"]  and w.text.lower() not in stopwords_list])\n",
        "    if idx == 5:\n",
        "        break"
      ],
      "metadata": {
        "id": "qgVCsYwTAX8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "We apply this filtering operation to the whole dataset, saving result into a new column named `text_cleaned`. How you can see, we only choose POS tag that correspond at one of the elements of this list `[\"ADV\", \"ADJ\", \"NOUN\", \"VERB\"]`. \n",
        "This column will be used for the next pre-processing step, which is vectorization.\n",
        "Below, we provide the list of POS tag used by `spacy`."
      ],
      "metadata": {
        "id": "L6q3eVffLvyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |"
      ],
      "metadata": {
        "id": "fnSX4FQPPi9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize empty lists\n",
        "text_cleaned = []\n",
        "# iterate over each preprocessed document\n",
        "for idx, row in df_data_bcorp.iterrows(): \n",
        "    # keep only the lemma for each token that has been tagged as one of these POS tags [\"ADV\", \"ADJ\", \"NOUN\", \"VERB\"] AND its lemma IS NOT contained in the stopwords_list AND its lemma has more than 1 character\n",
        "    text = [w.lemma_.lower() for w in row['text_preprocessed'] if w.pos_ in [\"ADV\", \"ADJ\", \"NOUN\", \"VERB\"] and w.lemma_.lower() not in stopwords_list and len(w.lemma_.lower())> 1]\n",
        "    text_cleaned.append(text)\n",
        "   \n",
        "df_data_bcorp[\"text_cleaned\"] = text_cleaned"
      ],
      "metadata": {
        "id": "4DSCDScJee8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Vectorization of lexical features\n",
        "\n",
        "In the following section, we create the <font color = 'E3A440'>Document-Term Matrix (DTM)</font> that will be used in our model. \n",
        "\n",
        "The vectorization process consists in transforming a collection of text documents into a matrix of word counts. The following chunk of code is used to set the parameters of the matrix that we will create.\n",
        "Pay attention to the following arguments:\n",
        "1. `min_df`\n",
        "2. `max_df`\n",
        "3. `stop_words`"
      ],
      "metadata": {
        "id": "-A2m2Np-heiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 5, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 450, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords_list, # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "rh0pO9WIgApa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the following chunk of code, we create a `DTM` with frequency weights. Thus, we assign the result of the vectorization process to the variable named `freq_term_DTM`."
      ],
      "metadata": {
        "id": "YWC7liMKMr62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_term_DTM = vectorized.fit_transform(df_data_bcorp[\"text_cleaned\"])"
      ],
      "metadata": {
        "id": "fZru5J1zglsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Thus, we print information about that matrix."
      ],
      "metadata": {
        "id": "qb2iPmcjTP3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_term_DTM"
      ],
      "metadata": {
        "id": "UzS-0TEEjveo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the next chunk of code, we create another matrix using `scikit-learn` but this time we use the <font color = 'E3A440'>Tf-IDF</font> as weighting scheme of each word. The Tf-IDF is calculated as follows:\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "&\\text{Let}\\ t = \\text{Term}\\\\\n",
        "&\\text{Let}\\ IDF = \\text{Inverse Document Frequency}\\\\\n",
        "&    \\text{Let}\\ TF =\\text{Term Frequency}\\\\[2em]\n",
        "&    TF \\: =\\: \\frac{\\text{term frequency in document}}{\\text{total words in document}}\\\\[1em]\n",
        "&    IDF(t) \\: =\\: \\log_2\\left(\\frac{\\text{total documents in corpus}}{\\text{documents with term}}\\right)\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "Then Tf–IDF is calculated as :\n",
        "$$tfidf( t, d, D ) = tf( t, d ) \\times idf( t, D )$$"
      ],
      "metadata": {
        "id": "bTzg1vPTfKiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, we assign the result of the Tf-IDF weighting to the variable named `tfidf_DTM`. "
      ],
      "metadata": {
        "id": "rXuvjwoeU_cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the tfidf matrix\n",
        "tfidf = TfidfTransformer(norm='l1')\n",
        "tfidf_DTM = tfidf.fit_transform(freq_term_DTM)"
      ],
      "metadata": {
        "id": "1W4puzyXkA7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"desc-stat\"></a>\n",
        "# <font size = '6' color='f28c00'>Section 2: Descriptive Statistics</font>\n",
        "In this section, we explore the cleaned text of the web pages through descriptive analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "_jutXQxXmMtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Metadata\n",
        "In this section, we extract some statistics about the following metadata:\n",
        "1. `sector`\n",
        "2. `industry`\n",
        "3. `country`\n",
        "4. `state`\n",
        "5. `city`"
      ],
      "metadata": {
        "id": "JYFvlJB9prTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We count the number of companies that we have grouping by the sector\n",
        "df_data_bcorp['sector'].value_counts()"
      ],
      "metadata": {
        "id": "QVmYHQylSkW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We count the number of companies that we have grouping by the industry\n",
        "df_data_bcorp['industry'].value_counts()"
      ],
      "metadata": {
        "id": "e_ahE5nrWVHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We count the number of companies that we have grouping by the state and the city\n",
        "df_data_bcorp.groupby(['state','city'])['company_id'].count()"
      ],
      "metadata": {
        "id": "9gD5J7hdUXB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We count the number of companies that we have in Canada grouping by the state and the city\n",
        "df_data_bcorp[(df_data_bcorp.country=='Canada') & (df_data_bcorp.state.isin(['Ontario','Quebec','Alberta']))].groupby(['state','city'])['company_id'].count()"
      ],
      "metadata": {
        "id": "6Go1I-uRU1Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the following chunk of code, we do some descriptive analysis of the metadata `overall_score` that is a score assign to each company by the BCorp organization. The highest is the score the more virtuous is the company"
      ],
      "metadata": {
        "id": "RKhqXDKTzkhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get the mean of the `overall_score` grouping by the sector.\n"
      ],
      "metadata": {
        "id": "2y4Jv6wUqwFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# groupby() execute the grouping step and .mean() is the function to average \n",
        "df_data_bcorp.groupby(['sector'])['overall_score'].mean()"
      ],
      "metadata": {
        "id": "aldDZhJ_lodG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the next chunk of code, we create a box plot for each sector based on the `overall_score`."
      ],
      "metadata": {
        "id": "vjiTcy-t2_bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_bcorp.boxplot('overall_score', by='sector', figsize=(24, 8))"
      ],
      "metadata": {
        "id": "BcjCQe-k4TsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "We calculate the average of the `overall_score` of companies that we have in Canada grouping by the state"
      ],
      "metadata": {
        "id": "GC2nFFBJrJxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_data_bcorp[df_data_bcorp['country'] == 'Canada'] this part of code allow to select rows which have values 'Canada' in column 'country'\n",
        "df_data_bcorp[df_data_bcorp['country'] == 'Canada'].groupby(['state'])['overall_score'].mean()"
      ],
      "metadata": {
        "id": "zoeGASGXXAiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Compare two values of a variable. i.e., two different industries of the column `industry`"
      ],
      "metadata": {
        "id": "kylMSdANsHnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the industry sector to examine\n",
        "lst_states = [\"Ontario\", \"Quebec\", \"Alberta\"]\n",
        "###### Setting the parameters of the graph to plot\n",
        "for state_ in lst_states:\n",
        "  # Create a subset with the two sectors\n",
        "  subset = df_data_bcorp[df_data_bcorp['state'] == state_]\n",
        "  sns.distplot(subset['overall_score'], hist=False, kde=True, kde_kws={'linewidth': 3}, label = state_)\n",
        "## Plotting the graph of the two industry sectors under exam\n",
        "plt.legend(prop={'size': 9}, title='Sector')\n",
        "plt.title('Density Plot')\n",
        "plt.xlabel('Overall_score', size=12)\n",
        "plt.ylabel('Density', size=12)\n",
        "plt.xlim(0, 200)\n",
        "plt.gcf().set_size_inches(12, 8)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0P-o_7lf1QfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Textual data"
      ],
      "metadata": {
        "id": "NkMeBy_FmvCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we extract the word frequency from the cleaned web pages and we illustrate them through the wordcloud\n",
        "\n"
      ],
      "metadata": {
        "id": "f-cbYZiO477h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the next chunk of code, we need to execute some intermediate operations to prepare data. Essentially, we compute total frequencies of words using the simple frequency weighting, which is stored in the `freq_term_DTM` matrix. "
      ],
      "metadata": {
        "id": "nci1xraEa9U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get vocabulary\n",
        "vocab = vectorized.vocabulary_\n",
        "# compute the word frequency using freq_term_DTM\n",
        "data_WC = prepare_data_for_WC(freq_term_DTM, # Fill with the DTM\n",
        "                              vocab # Fill with the vocabulary from the vectorized\n",
        "                              )"
      ],
      "metadata": {
        "id": "FMmI4sX_711B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Then, we generate the plot by using the result of the previous chunk, which was stored into the variable named `data_WC`."
      ],
      "metadata": {
        "id": "6CSnV3ou6Xwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and generate a word cloud image:\n",
        "wordcloud = WordCloud().generate_from_frequencies(data_WC)\n",
        "# Display the generated image:\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HEJ_7ToIdYwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Below, we show the wordcloud considering a specific industry."
      ],
      "metadata": {
        "id": "kIzdPakpceoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vectorized.vocabulary_\n",
        "# select some rows according to the index corresponding to companies in a specific industry\n",
        "selected_index = df_data_bcorp[df_data_bcorp['industry'] == 'Marketing & Communications Services'].index\n",
        "selected_DTM = freq_term_DTM[selected_index]\n",
        "## Create and generate a word cloud image\n",
        "data_WC = prepare_data_for_WC(selected_DTM, vocab)\n",
        "wordcloud = WordCloud().generate_from_frequencies(data_WC)\n",
        "# Display the generated image\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.figsize=(12, 10)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s_xIFZdv9bsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following chunk of code, we compute the total number of words contained in each company's `text_cleaned` column, and we show the differences between sectors."
      ],
      "metadata": {
        "id": "UJjmFznSYxbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new column 'number_words_webpages' with the number of words for each article generated with the \"apply\" function apply the len to all the column   \n",
        "df_data_bcorp['number_words_webpages'] = df_data_bcorp[\"text_cleaned\"].apply(len)"
      ],
      "metadata": {
        "id": "PnywKacd4HJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute code for plotting."
      ],
      "metadata": {
        "id": "kF-LjVu64KqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the parameters of the graph that we will plot below\n",
        "for sector in df_data_bcorp.sector.unique():\n",
        "  # Create a subset for each sector\n",
        "  subset = df_data_bcorp[df_data_bcorp.sector == sector]\n",
        "  sns.distplot(subset['number_words_webpages'],\n",
        "               hist=False,# Boolean value to plot a (normed) histogram.\n",
        "               kde=True, # Boolean value to plot a gaussian kernel density estimate\n",
        "               kde_kws={'linewidth': 3},\n",
        "               label=sector)\n",
        "# Plot the graph with the distribution of the number of words for each webpage in the different sectors\n",
        "plt.legend(prop={'size': 9}, title='Sector')\n",
        "plt.title('Density Plot for Each Sector')\n",
        "plt.xlabel('Number of words per webpage', size=12)\n",
        "plt.ylabel('Density', size=12)\n",
        "plt.xlim(0, 1500)\n",
        "plt.gcf().set_size_inches(8, 6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nJ0UUOFfYuN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we execute  the same operation but considering the `country` and a specific `sector`."
      ],
      "metadata": {
        "id": "KO9_7D11w1Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the sector to represent in the plot\n",
        "sector_to_look = \"Agriculture/Growers\"\n",
        "data_MEF = df_data_bcorp[df_data_bcorp['sector']==sector_to_look]\n",
        "\n",
        "for country in data_MEF.country.unique():\n",
        "  # Create a subset with the different countries\n",
        "  subset = data_MEF[data_MEF.country == country]\n",
        "  sns.distplot(subset['number_words_webpages'], \n",
        "               hist=False, \n",
        "               kde=True, \n",
        "               kde_kws={'linewidth': 3}, \n",
        "               label=country)\n",
        "# Plot the graph highlighting the differences between the countries of the selected sector\n",
        "plt.legend(prop={'size': 14}, title='Country')\n",
        "plt.title(f'Density Plot for {sector_to_look} by country')\n",
        "plt.xlabel('Number of words per webpage', size=12)\n",
        "plt.ylabel('Density', size=12)\n",
        "plt.xlim(0, 800)\n",
        "plt.gcf().set_size_inches(8, 6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8hadxGPGXu99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coHj_8ZazkCY"
      },
      "source": [
        "<a id=\"analysis\"></a>\n",
        "# Section 3: Analysis \n",
        "\n",
        "In this section, we run the *k-means* cluster algorithm on the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Document clustering\n"
      ],
      "metadata": {
        "id": "dmlBLX-HQlCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned in section 1, the goal of the *k-means* cluster is to group rows in a homogeneous set of clusters minimizing the pairwise squared deviations of the data points in the same cluster. Below, an example of a *k-means* clusters with *k* = 5, where *k* is the number of desidered cluster.\n",
        "\n",
        "![k_mean_cluster.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQAAAADFCAMAAACM/tznAAABZVBMVEX///9PG2NOnp1QZ5n76FqGzH7p6emqqqrz8/Ofn5+MjIyAyniDy3t9yXVKnZuBynlJYZVFXpQ9mJb6509BAFj65khGAFz5+flBW5L650xNFmHv7+/m9OU6Vo/h4eFCmZjl8PDNzc3W7dTw+O/W1tbBwcHC5L+4uLiz3q+Dg4OMz4Xv+O5sfqc7AFORkZHI396i153++t9JDF5hYWFurq2sz87P6s399Lv76WNkqajO4uKPv76m2aGS0Yy/xtdab56Xo7/763b++dn+/O3Rx9VWVlZwcHDa3ui7wtTW2uX998yhrMX88qj875X99LdhNnKdx8Z4iK377YW3p753V4V2hqz88JuNc5mbhaVZKWxsR3ykkK1HR0cvLy9Ol5xPfJqyormCZI8mJiYGBgZnr5tptI5zvoSGvrCt1r1PjJpadpSFmq9PcJlpl4xegpJ0qK/KwWrY06B9too1TpCXmoGTrbLJxIlqKguJAAAgAElEQVR4nO19CUMiW7LmERUT2RdJBNkXAQUVQUVEtKTEtdzLre69VTP9eub23HnTr3tmfv9ExDmZJJAkKKjv9lR03yLJkwnEd2L5Is4BGfspP+Wn/JS3F8liWZT5YUQ9OSvOMDOM0sGiJcWK7bval/7pJbJuTv8iM0li7JtZZiE6mWLiKF4M0eO3dMQif4az+P8Qkz5L+EBXSaEP/PRjkMghYzMz5tlZi/xfZiMzxXWc3M/s8PAQJzz+2YL6pWnyP7NvTFqXPxct8V+K7LC4Ls18Li4e5uIfqsCoggDEi6yY+4Wtw9Tn1i1wcp0dhkBbcBBz5Bd4mCE/+Az/kw/Nn1N4QXx9MWeZSTNL8V/AAorpYhz0BQC+sRRONgAggbYkaO3xQzqCYXmdhWY+wwXpWXNEtsQRM8sHfvzRJfJtMfeZzeSK/5XlLOZvYNQMVV2XyAIWLcV1vOowN7Muf4MrcuuRIgDwbZF9m7GkAICZmT85AHIqZYaHSAj+TYVYSqZnzMwjvZwS8d6cohM4HIFDCac+wkISPfyUn/KeMvOvLosDAMiFzP/SEsoNAKA4YPxPL7Mjjv/p5ScAI47/6eU/JwCVy9h7vVWngpHZCIvPYmZIzfIq/UMAKHuiDnnwZWORTgVDkBYjrAjscyalN/4+InscJs/KO71Zl4JpnH0oTlk8BwUKmy1+iAVUow5P7J3eqxuANJardBif0RkfReSV2JBXSrXytt75lZru6ZGkywVyOWnmMC6lI/E0NWvGCIDs8Ixo1xWPwzN2BDoVlOJxKZKKSyFJiod0xkeRStTkqI70CmWHKVob06dR5f3S4IrH4Sj3G5SGeQW0gLHHxnfkAbVo+bJc0RuJmTx9odFKRf/2keRdiRAYgUdPhfI7pr1ueVcAahAG1KmObceqJg5HFZx7/HM7nHyYBax4osh3YvzYM1p4HEHetxZYqakTDbMOIixfHn9+H1Y+rBgqOxzRqMf0Vi8/tHwYALFqtLa9Ir9JZH+JfHQ5XOvJ7TFDTrBdG3O6+GAAZIfD5NCwO7nWhzDHKhQmtiF0jtdiPhaAMrDDDgu4jDo0hFmKKUcxofilNpEytpRZGPUjfCgAFY/JYSprJ7wGuUEFALKjoqxSSGAivVSvXrBb7YkRP8PHAtA5n4ybuCkmnrTTJCgu6qAVbczMOqdtqyN+hk4FU+spJufoHP/3bQHACrmLAcRWYpeKTYB5qMMVU623SbZhn7ZnxXFytb7xms/QqaCcXmRFM7aFLKmxN0T0ZLs35NfUqn/b4xlQ/WZtO8ph3WmzLy1kki/9BL0tsUNmLjJ2KEs5MIPcCwHYrgxqZsqXA9o6YPhqYfCCzqh1GtwBYkIvAsmE0cv0tsQOWcQCqkvmos74AKl4PJ4BpX15UL/v0oMXxFZ6P7VkhFzWbs3np6edO90DC3a73SBXdLXF19fNqcN1aTEeOTw0944PksFlHRSEpij4uQGdqYCJQAJwdCOJJZMBBIkM23FOT9u7IwGcdGZ17yDpigGSJIvdabKsMz5IylH9wr49m2DgjihWgrrerQJT5khuxxgnzXgOyydjFiTnbaqyWWt+QRy0I6WOjDcNSmXTpc7psqeqTCeGuFgFzaAa67kO6Q6/nzoHtbLHA4iUBVdCADqz5kK3aW9AEODnEvZpW52O5FXrqkEQGBmAWGzQFdjLazczwcBjQGdAl14jR17g4K8K2gJJxKuqcEgTHzNFiQVlskLvjN1e73qFRGaJH0CGtE0P/GRsdAAuB6aq7QrQO83MxUCnCikX7bCWlYq8DUipF5qoYQAQ1DweE5/CShXu2LE77Zz92MHjFRtIiOgvZ0QMyNt7ooGujAiABGTWOKiDvzsUvrOClkzuHSuDfg6TJqahmmylqtCdS6E9uH0s1hH6IN9NcwIMcywASNrsghPbrHaRBxLDlQkjAiBDNWOc+NCWLznfKRO3F/FNqmLCb1/nafNeaSW27aH5d1SpbRjTvuCqVQEgMW3N8HMZq+DECbSKASp1yqguAJl/YN4TisVQxxi6QFl9yuf7sry9zQdRIN15sCiCIACvXcGKoBqrqMlFzlrt3QUAePz0NFr8kt3m7I4LxjJyEJQHsDVZTQxyVMQCmbcGy0r4wMwAgVJUgTEPXlZG84cUEPNQuOjMmwrby9pWkzJ5PzGA7EaSJeqrL2PD71ANqggpQV5pDm8L364i+YXBGHh+ja1E0fQxcGLDEJ/xaODoeWGwd2d9mvO8HRuwwE4inMgOUyq/OQBS1VMVEChpvqbtAgEKl1HBH6Ho9dQuufWjBcQoYqALRCk9LO3saDXMIP+H/++wJXACJxwqMQEFzgzTLHhzACAGqOmOe7KsXR4gPwBEomj/l1GuuiJRAsBRjrGVMqYHm9OZ57dlV0G3JLD8unPalse8mMhiPajReNWmUxi8XMHRAXB05nsZLOKytiKWAjASRsnMMVGCiZhUi8dzUgxPKCxqAUM8mUDWbiPD31hgq87Vac4Hkqt5jQEATbJpmUC/UDVWAPTyIdZH2jxeod5OrewgdhPDphj1wSgFxC7R9aFa8NQ4ewKnQBH35q1WHuKB84O1b3BKmLAJABJLHe+cWdXgsWPPLyT1wuM4AYBo3lOtUejX9n3RzKtQE4uqvwJsiFqD5BPbVZj5qqlakcFyMGPIHYwxmc2SDpjtrUsZcPIFbhfYF1mycjK0pNcawjuALGV6BnQUjOcOsSOUK+qPb+sU6lxwtnp5cRnnUMMVJFPUxFkuAUCrADWYbHyUPGp4VFgC8iVHd4FF0W+nDnE/w0kQRoasIEPUGoLHZN3aLgKJKCgONAAA3CAN5+P64xWVmOsCYOrdw1KpmjqbBHRdFG3AtF2mVQBZ9Hxj3ODL5eq2hBQT30gTREElHtWSdhvFf9I46+TlLuhIw3ZBFFed2t5A3m51Dg0Afm0lFcdv7/S2xMAko/2aGdWoHgC9zQ2gNo5KbBtrYp7tkQZGZXoFB1WBoDtRTHlblimNErFvV/ZJCAI2QmCnbuUcEMlQHfXL2u1UBtZtHVlxIZkZ0gXYIt9CPxPXGwcn7b9RqaI28leiMOkVMe+xLsRiZdwEtsJzHk59TOXBlbJYNY4hxYwBKY5h/K7TZwebn8YoKCfkBbt9NavYgbVzXhMZMtEslEqdUTGpZ7o6AIAB5MyW3KHueKxq1PNSmgOyB3WLRrtX/du8GSYWIj9Wypj+HMLecYCnQFoCFIwpYUWOg6GfEl6Smnyg9MYqnbHZ9D5KQnGFAfI2PECijoda3igCFnKpHoKvb5ch5NfkGhxXFSvZhuhfwzqoVqtF+TIRam7LLyhxrL0ckuSngAdOi/7XUj0v/L5u03QLRlDwlUSoRowm2rn8r62dcZEPNER/p+aPepHoAph4Q4hyIOjoBAXR4qdBvwxEOx4KNrA3MJ3fsTltNhv5NxBDsvsEgqbn8i9W8OUAcFNGrarAYbUikavLnA+bPI5YWWgKzEfcuxKrqp0QcfAXmEua8QWkPwng/av2Ovf6LLnANP+X4qPoFfA0YdAKHV7BoQGQYpeXEu1442t4nvbuj1hMjhEsFezpOURlC6e2kfpGIRCIxAKoRHkQdJQV/SnaWymaoVvn6/Y8h3ghmyDN8xQbp3kjZMdGncAlDACdERDsJaNGoMyOGh2GBmClrNfvVQXcm2JeVdnx1u5jQa53wHTj5obqdowWBtA0wAm2kfpGYxADL2OVGA8MYv5joir4N1KS97oT3OW5/S/YneQAZALODNHBBfAJK/n9qr2nabJjt4tKCjKlUw0PwwKw3bEu3SMytbAg5mlWdFVoUCHgtchtTFHu2GXqdJiwF4Dc14T7hVZE8R+NOpQAQADYVJ1xvpH9JBYW8ta29dtWk6ATpoO8WhEnehKApoWa11CEYQG4NN7pKytFS++ONz6toOy2p13oVemZZwXvE7pC3C9TVbSyUmlHgOhfnXWwes5wlAm32oXfOzem7U4ixCyJ9BgcoG/qy7dXiJRi8iUAxPps8lQEaxyq+jpbuJVaZaXqEaUPTDPaQg3cBMgdzvUKb3hoSmDgiHybBD9TXvlvC6IFllT014hdlhMZmM66jNQXtFrtH/mWVm3TophsLy28IAYM2p2EGa3nJNSHUU8lxsM9FLuV7TLXGCribUfUVK3WNAAQBpXaJV5OZuGI/vcdlb8KHkDTPM1J0bR1g+dCcpHVHRH21FWxLgTsesXAWLJADKoX4LexngHSoizLaO9VDohIfdWaqbYd7ej/4OnaJUKGpytQBNXynAOSiBDoXE1mbdbEhpN7/xLRQdvOxgZwBR7bF9BBrEohlMyqFZHV6exdLBoLAKY2k0WRVSQuiRDyLjdQoJhHM9WOaFnj6hQcIcqIjmCtUjE5KlxnZLoL9XoiTwDA3MKpepYAmK5brTwOWjEocs9WkgW1h5MQ8JXG2IJqI+MCYJtripFMUyFtU0kkr0hsG+N7zSFoTUwNg0LrS2X+HabLCmoOkaGqIgJP8yLSJxnYPcQwXv+iL1gTdjUJasRWRwSEr9ACATYCnK9WcMB4VVkUKXPWgzozvrVnJRYFa6fCP+bhk2vydDk8JH+R7DHB8lTJOCYiL9gF06vzWc5gDgQXwAAgJzfyTlsnCE5hA7hIbhe+s2S3WQ33UfUqGErRN+YGf21uxaNy+BXcGQM6e7BdRHubLhX/duDiMALg0NBbnvYqIgdgx2eFZ5HdP36D6379jfLB73/7B1m2yHniwboDdbENXGFpdZWs3QaRwqoAhPl9oZ7PLOUV9rSaNVwp6VUwl8ZGQG5xfRAA25zKqIJr+lFwhe0ouEBFTC52CeWOmY9GCZuoqaYwP2RO4B2OXxt77knXb7/++sfk7wDCb5OT7v+RrvfYeZ1h2OPxHAIDeEU2i6nAnsw4ORFMQJaD4/wwW4x6FZydTcNnPmRF/JmPlJELXHpMXf1evoMBg6AS7yg44rJ3VdF/5fIytlKl8pfXjJQfWKX66x8u1yQI/rvcmgzjE1f4f9qEdbeT/wL9y9dH61QMZKkKWsDIyHhDfCGvs1tmOABkloswOccsEWwOGcaIToTlclSzEaAS5RUtPdmuVZQACITo0iOaKtwCorxCbjVIfRLXEbjiFSHw78K6bSoA1oxT6YIo5MiehWIgnxS4QMhwZrAlyDHK143aAjoxgOXMcbbO1l/+3WG53N7miQuBJuIGK5xDcj9wQO2DvSB6cbmK4eH3Fh4vB0Dbyau7ZUIgILFjjsffMJyBKrz7AwzIubqAfWDyAIUcAQIJJzUJsQZEpptI1vOcFNohgbwIgBlwAQsz51J9xo0hUI8wRJI1YMs3Wl3hayB4Dg9+PeZX7f7uADPfZQy8f7IBc+66ChAALXbHAXCl61Z0+AXuBfnsjg0qIjGpWABMU7LMZ2ic94OyqxvYQyYEsppmcCbfu1torA2Rs/mpE/VJzcO3QUkUGjxyxfHrHyYiDP/L8bs70MCxFjn6ZIPJYXhYvgN9r8xkA43jK67/Keiws6AoO82bwdgGQ8HYl1eMY7rdA8lkVnfqvDCyTqv7iROQEntWC8cDwKdP+O/a3NSUV3t2kx4wAf7xW6Xx+6SLAJBdDdTsarLROuWzHNhlYP7h1hUpzE+6RTS4Ul+ursn6lOKWMlnw9lUMeiJLUtiTWd7qpIoBc2K+3RzGJZSe3RNjAOCAXc/NXaPCAMB8+/zZ3PzUJ9baOy2bfgPbRnV+w1WyZWHbk67GcViZ5lZjclfCgbvJdiwkIHaV15MzeREFbdTuS+JaB/YC2Ea2jvM8TU0QiAAcD7vdliRSoCQDuKE3MYwOwDWoPTU1dwCH+3NzZ+0BOn0WdgVOr9o6/XHEwuqTANul0B+gMMh2A5N6goOhvVOZtUTzb2OVNKWWqNL/gOMkLX4uETeyOe2rSz2dgeRGbz4YGYDN+SnSlJ58WtOM7NOAa7JzSsNHDX7QmAzswVV7Llf46PgU9+XKrvaVHDO61H21y1xud2Pv7q84t+qiP6UA0f/IOm2rdYIFz0KkzAzTER4JgJOtfdL2CwCwtaUJfqqsITT7PMy53csNJceLx4brSN7bW16+ujo6CrvDLfAVM6QAF3d+SouTIhAGjvCky/XHP/6q7I1MLrElqoP5nIo1A9QaQqJNnwFlrNZeWF4LwAEaPh1tzc3paA9y4oWYuLaLLu/Gn8pc7rbt5YYb0RAjLlfgip0u7+2izUCWPG6oF7pO+ePd8t4SKbyUARPnwd/uzGTzGREaUT/ygc69ElySY22IYMAjv4fDg97h8/1NdgAAbEEs/Fs4DLmMMr0y/W6NV4TNfAhHwdmPWwCHJO+GegBzhVvi1RfsVjUfgto2mzUrOCITnmHTWRUaLwBsa54if48cIBzX3qm5T2wLksLa5tzU3/8DB8jsybXdk8d7KgKY5+QrjkDjai/gRkZ46goETtVYge4CKWNPkCd2+g9aGNbUB9YMrp5Yeb7bsTundQuBrF1n2/jrg+CmruFfk0OA6t4vmAbmT756p7z7OLIXmHSb5cak+1hmslud18aye48xgYASBV38vwYo79oLYJRcvjtC/Vunx+zon3+xQsG/ytdFnPZp/J5Iwm7PKxull+SM3T6tUwr27C4fCQBdOQHP2DqB6OcF/wDdp8BXvHOcEJ1eTS4foTe3WKidCtEilq+kXSiEe4IEguA6Oro6wrsDQB6BOYZ3wXr+YcX9cjvEdRJsATXvTHqJDdJ/iD2T4wFgTQkDqDtM/5T3/ACJ8RaOnX0Sg2D/6Pxo9HudbMfVOHW5KPy5uRm4BFuEw8ApVYvgGQHynGVIDf8kdoQ7ZYy2xG/omfxLFRwKgHOVAK1tgdUjLQClMUxqecFB2+uPlpXZhtjv0hj9JDCj3dNweBmNxOViBEN4D+5uIF/aDdNFgh0m7X1WwEUF3G94gIKRRfqhU7FPZBgAsAaYY1/nt9a+zuGkb3Jm7BV5AjMCyNe5//1PtxsVNu+qHtDYO21peKLghC109yOoFY5ErSCbT/caaDmQJtEoRDiETCAaQ3yhY4G3wJN2vkRKRGkQAr0KLkawEzBLv/46HADICbzk/Rj21kQmYCdTXrQLnhEO4GHq70e7V1eg4qmisusOLth1awBQNhOZ98DxjwQ3dh250S/cmE2pTOYBbvf/1Ou858F3xS7Ag5IIERhaMR30zVI9BQ8l3CaHv987oCOkCJQ9myecEU7Nd7MCnhH2vRgZAIZ9GG+5BMnDzg9rtQFQKx/J7QrsHamGIgCjEprduY/o8SrgbhzRrx3t8F2xarVXt/O1gIzYOPZCAMQvvFtSg3qCbcEpB0a4ed5ZD5OAW3gP0B+mpq6BN3vPQT3MbiEMeO7Tq2Vt8F8WN+22ibDbpfoIjraAWYYJJ7zE7Q5xRXHqod4XUW9DpAR1K8VLAJgpyiwtpUIv/t7gyRpmfu/JwflZx/kvoPzBV6oZCQWuHrC+I3c78ysuQKpJLTNkxSvyDZc2acIYlo+uPaiejwg4sAZp2fWXOjHfjdVuAjy4L6wTA2YsclyOW17TEttHf/d65/eVEwebnxAA7zU72VcA2GT/EXa5wftbXeUv6EtzewwpHw0ebaOBKhw1OCtclvm8Q0UJGZUiJNBjINLEtUGSO6tK0FO+RryR7V0OMwZgiPEDHfbPB663Nj/NK8UxPJ/3zp2cEQDEkUi8W/N///ciRjvV+HntezQZoP5PQwsK+fsx9wLXMgXQxpEaRRu7RC9ce/z98k6bnU86foMQNcfNIC9cGBk8LtIcyvl+DyE+mPN6FQvA0L8P0cGLfEC1AcwVdAWZtssdQBpEpQ7lgDutV1Dkv1NMBZLElQvSIfrGXUPkw1Yg4OYpC/dGibyn7BOd7twvOh4AvJzocAU7uA7J2vVX0PbsZGruC6oMquLFB2cnW8ICkCh9wSuXMbvtHYMW5qtlpdSRVI93N9wBl7S7fKVQJXeDmcNYPyIWodNwAPNhKyQfK66+829/Vb5RIfaK7Gg2g7wBAF5KcT2ydnAGZgBjBMAmRIZPm16vl+u/f35wtsXtRt672ju+mjztvBsCo0tMfuj4+CjAnzX2Jif3JGofBxi4P3jD8S4ovseLbXgpiJZud5hwTNZtNtERoO+WjBmAk/m5c370dc471xkPzrfO0OSnNEIMoH3m+uza623nCTD48HHHS7SurqhUwKS3G1Y6CBj2dlmLHTcax9hXFWywFebsCYwpIJ+6ObViO1baPDKMvK4WQKU3keBufu3wACB/MO0d6vcKDrfp0qRL0/gFMTcgoplR7wYEhWWFAvE6qBGepDCB7s/ZUAjJhEzhJADWMom1NbXFjD1/VAAYFv7zW51nDj4xY821oty0+X//5nJpfeCOeA62hTDm43IxRjz3sSDMXMGjcIDyo3zVgHGE4i7gAhz2XMsEUMJudw759cFReoKd8e/L3Pz+YMWFKLnjet67hcVO+2UoB8pUBjRIw1MztUVklwYAKJfoltPApEBPPt1raT7MUmJI/V9vAVNqT/Bsi5LiAMNXRdNBPjiHm+Zn3W4NALtU+DO5EQgrnnEUdoWPjykVuhodf2VGQwF0ZCM7sBgeAYC1fRHKN+eI358NCwDWROI1KBxM/a0jBgDXdYVRy+O2qrt7u8CHeR7skGPsp/f7hD0caEPvq6SjN0TOiOywraH1B17IEaDW+tRWF5UKtSlBh0C+XD5lrU59zUfmvh8s3xUJM7pfJR0dgDXe9ANONLQNeEX02KLsKFqGA+UIC6QwtYdQjq9Oja7u5UB1m96vqYxjcfQLsaLz/bOhg6CXU+mDr2fX6pO2HO/tdr+HOaSWCGGxoSTsoqU1A8l2foEQd84MZQGL9BNSlnS/8R5Rk8HwAJwrt271WsAxxLyjzlNE98TySZhHTFotG+LDaSSzqkOOdMrhRWwHFlOzKf3xbjnYmpsXfd/rIZ1gfl8Jg9guOjvY8mo7CEB5OJ9TBRsCMPG0yKZETLAA191pj6m8WHS2yUlyDr86lgJDsFiGC4IKJRrSBNo3YxrEKtG7v6/aEdDfcKd7IwBuCXR2a0yjtYcLywPiwGDRAwB/ReyQAJCkwQBgw0fpgw2VC+c15dPZ3NwW54/eNixHd6DW7t1emx2cUvHDQkcdOQA7IstsROlVMD5bjC+GZmaGboltzc0JlQ6uB2rv3Zo61968doIoUDLoqKrAwt3t3THHbk4Bj5cbmhS5Z0iDhhQdBVMpFpFZfPi2+AlZ78HZEHkQWwW98ulkS9NGI6F+ofoMVYW5lnFZof3dy4C6t2QEGdva4JqmBvb2jQX7/W4/62orhAI8y0FVdEr2gN7PuyHqNe6OZ6+UsQFw7uWeDML7gvoecKa7nYBEXWBcuz4/gBhH8Q6b4C1cFaZ4fxcIaJx+1x0YOQaODwCKf8CFvOTMerpTY+Rsf06H+W2en+ACo/APr9erVtoupVXOZbcj7w36CZ9hZHzL43wp8OCa2qTnPeEAAuWad25rEwe6b/0CbPqLspTIlxoVaxCNv7eT8QHwacurmVpEwDtF26SwTNrfJ6qzRltrtrpvxZCBJEr0iaY0FsAbf28oY94g0ZYv2Dv/JAxhvl3xfZ2b+tR97dnc1NzJl3Y2/fr38f1t3dZF4d5o/M0A4KsnaAfnSHaM5ey6Iy788BV+vPp9u+TWP1G4MRh/OwBIztAODnqm3FjMhYmJwsgJXshFcML33WD8jQFgB0CSvugsHRiJ5AsGfeP6A8s3BV/Q6LXeGgCQ/Xlvrwt82T/rOtMiTiPfXzyzm1LJyGpfJq2mYRR9ewB6+8eMGolzZ2tflGQn3bTuC74fAMGDL4gu+15/a+5dLADTYfe5r3y5SKR7qeTzlSYm/I8Qs0oT/ofR33N4eQcA1vb3e6LgyZwghvTsxjdB4jOzH6WJkmHaGrfodIRyuEUm/ca/Lv/p6wlwpbnNh9JtiDULExD2/L4nxp4LwZ6gLd9PvB0mvQqa8Wtz9K1B/fHxyfXU2U0h6IeU/+y7bYWev6PnP1zcNrt+lQ7jwndmfnjA899vv4/1Q3QoaI5EIvAhPsN/6SJulnvxr8u/VJ79E8EnPLgvPArImz6fDw/VFYAQ0ILSA/MLCykBGGOUDgXTi4szEjvkf+B9Md0zPn4x+3zE024KE6UnPvFPwQn/M0x7ofDUBDhuWg/+CUDpMQgxQv5emFAQ8z8N9SfqBolOTzC3KM+YFxfXJf3x8Urohnpc3yEMlnw3OOn3kA98NzJqWjA/FHw+P8bHIOgP4fGJHiBbABL+sQSGXgVD4AYSkyOv+XX5F4ssZlHCCZ7wF+6bTQQg+AjZAOc6ONEhpQk6UWgKmMbwCd4hDRrIjd/3zI8eSlxDIgSou+bftgTvL3i6RIsBmMbwET4UANkXFFVP0zcxlHBE/E06KN2O4TN8KACSD8u+m6cH2d9P4W4b4GefHskQjtnN048R26LvBoB0f9vsOfmj5LtvFkq+H4qewW5Nb/URoH99zRbQiKfRPte7AXDrK3XX+NJjyX+DVGDi8bHEfaA76hk5wkRBQs/xj/a53g0AX1fUboLqPkhszQuK/89N0upxGP0hBKCUmkzy+40bXoPl3QC4L/iVJod067uH6vcCGE7plnt56UdzKN05ADdkKFg1hp5HTYXvFwS/Pyge8AAzT2n8ovB4I8JfcMg0wBEg0ILBcXysj8gCt1j7B4MQEswP/cL/EDKesvkjAGgCv715egTjbRVer/+En1ik9PCjN7u8QD6EB7RuMHmHnoIXQwV9HcGA4ScS+QTZZRQq8IFE6LHUQ3VfJMFHCCpIoXyjmICOgh1LjuMHQH7m0dAszF+f7A0ULJkgibIfPr9eC715OyRF7FWwWMQvzKUOD1/+O0JDya3PT94r4n/wUZ/sDdIfy8GJUuGYPT/orKGEfCX/cKWSDgD4E1LskMUX9cdHFCz0yWZvRf1HizcvFl/oCfDAHdEAAASJSURBVCqpCf93/Xdp4uLSUA2TDgVnisWiZDZjS4j/1cHZ2fFbwIUfCPD9M/DAIE3iw/fh+G+n+B/k56fSREF3Vy3AXPL7hqsV9RScicssh399s8/4aBK6/3HjgyKIPTxSE6Dkf50LMJndTvQNf6LFOlh6FbTkZpklEjrM9Rkfg9z4OI1rvmbuhUD5dFEqjb6I/DE8oIAWwG4LI6VBTKKjLyJ/DA9oQgzQ0MDSqzEojLyI/IFEKFQQFKB0/xpGRAXh6G3Rj2yJ3QQfsekN4aBZeLERBC/8wXE0xj8IgOOm8vhcemxiSfMiBCCVllrfL36ogf77fZ98OFA+BoDnQuEJlL7HEHb8HcF4fkldHLxo/Sj5H54ulCx4X/C/tiL6EACwgik0n4DFyxAK/bg49rKE6LuBcrAUDCpkrxR89SrJx1gARL8CsdUmLY9e4FJfsMQxGNgaQwbc5B7ja/L9Lz96O67DygelwZLvGSp5f6n5fF8KIidgPx6f+1hBd6v84hH+Aycq+XxBv6+EiVB+0Om5DycfmAUgBtwU/KWLi3uYxYeJJ/kZ+4KPPcthGlywg0CzXzg2t1gTmaThHrgh5GPXBtXtAeAP/lv2hEX+hS+onfdg8DvFx9IjFQCPwdI9UGBBgAbtghxCPhaAZiHooy1RIhLA9PuaISyP1Xl/NFOC8N8/8qUg/Pf2SRi8+WLkLaUfCwD7/vRAUQw0D97SrsaL7z/u4ZlfiYU+6QHNRMJLCk1kjz7tC4y8n05nf4DZjH983jz8V2bGIEiK0ZtbzZtCyf/jwv/w3QeFMnp7yAzpIeiXmrQIdBPsXwK/SnS+NLX4LYJfmBv+t8TGILhRjvf570sTUOTKmBZ+NO99uH9AnqA0L49r92yH6CmIP6xvwd+YZ2bzOwEAxu8v0NRCWYBxrXVBW+NCpPStP1gY77y3pUNBS242F2Jx/OqszPCbozMz7wUAaz4IFZv3GNex3aXqbL6deH6r99VR8FBiEVlmFt0/uPhe8vh6cvsy0VHQwtiieTE302/8XQR3ub/LjukPToP9ZcAu97HJf1oA3kt+AjDi+J9eBilYfJdP8YEySMHPln9x+TwAAEnOyZKBmItGw3LO6F4pNWN486HhaDo9nnceaCM5w1HZYjhs7GGRmRHeOW787VLjmyOLhsMdMiAdGw+/5c2jvPQ7fi3tzy8zRqYmzRhbsTltNBqyGL62waiMnyre971xJG3pVz5LM/gLQcYfvC25SM5gySESSRsGgcNDw1HDEn9dykX6jUmLOZaaTfXLYal1mcWlb31GzTNY7vwypBesi70y/SRthORiyigWpb7lDG/OGaAn5RjM43q/Ydrg1DfDpSxgXrNDA5A2AiBiOMXfFr8ZzHG82F8DhkqIH3fWkyEAWO/71gDA4cxnQ+9sSzF9aKCD+VvEaE0ukvpshPN6ygg+GO0PQHo9ZD5c7GdA5s9xtr7Yz4HkmZxkjqz3f/GutzLS0JxeNH4dw9GQ8SSk+4YAJi+mUyzSN0jG02mWTvd7dWkRX3lY/X/KT/kpP+X/A/l/zNC8x7XRZPsAAAAASUVORK5CYII=)\n",
        "\n",
        "(https://www.ml-science.com/k-means-clustering)\n"
      ],
      "metadata": {
        "id": "LfJmHL6KdA55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the chuck below, we execute the <font color = 'E3A440'>clustering algorithm</font> on the Tf-IDF Document-Term Matrix (`tfidf_DTM`), choosing `n_clusters`\n",
        " = `20` to generate 20 clusters. "
      ],
      "metadata": {
        "id": "WD3qeGa8yuZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_kmeans = Clustering_kmeans(DTM = tfidf_DTM, # Matrice to pass under sklearn classifiers\n",
        "                               reduce_lsa = True,# a SVD is executed on the DTM before to execute the clustering\n",
        "                               n_comp_lsa= 300,# Number of component to keep after SVD on DTM.\n",
        "                               n_clusters = 20,# Number of cluster represented in the plot\n",
        "                               random_state = 8426, # Determines random number generation for centroid initialization\n",
        "                               max_iter = 1000, #Maximum number of iterations of the k-means algorithm for a single run\n",
        "                               n_init = 100,# Number of time the k-means algorithm will be run with different centroid seeds\n",
        "                               tol = 0.0001,# Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence\n",
        "                               init = \"k-means++\")# selects initial cluster centers for k-mean clustering in a smart way to speed up convergence"
      ],
      "metadata": {
        "id": "5f-NGOycjFVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of documents for each cluster. Labels for clusters goes from `0` to *n*. In this case, form `0` to `19` (beause we have generated 20 clusters)."
      ],
      "metadata": {
        "id": "Q1_zbLVizeh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the Number of Occurrences in a Python list using Counter\n",
        "from collections import Counter\n",
        "Counter(result_kmeans.labels_)"
      ],
      "metadata": {
        "id": "eZETT6-XplUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Plot data grouped by clusters"
      ],
      "metadata": {
        "id": "xrZcx8ugkvlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this subsection, we plot the result of the clustering in a 2D representation for a first look at our dataset and the partition."
      ],
      "metadata": {
        "id": "IU_3NTXadm0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data_by_cluster(DTM = tfidf_DTM,\n",
        "                     cls_kmeans = result_kmeans,\n",
        "                     figsize = (16,10))"
      ],
      "metadata": {
        "id": "jjyNGiU8fJJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Wordcloud by cluster\n",
        "\n",
        "We use Tf-IDF Document-Term Matrix to plot the <font color = 'E3A440'>most important words</font> for each cluster.\n",
        "\n",
        "To plot wordclouds images, switch to `True` the argument `plot_wordcloud` of the function in next chunk. \n",
        "\n",
        "If you want to look at a subset of cluster, put the labels of cluster into the empty list at the argument `lst_clust` (i.e., `lst_clust = [0, 5]`)"
      ],
      "metadata": {
        "id": "2TSBo--psyda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud_par_cluster(wordcloud = WordCloud(), # WordCloud function. \n",
        "                      DTM = tfidf_DTM,# Document-Term Matrix \n",
        "                      cls_kmeans = result_kmeans, # Insert the result of a kmeans clustering\n",
        "                      vocab = vectorized.vocabulary_, # a vocabulary from scikitlearn vectorizer\n",
        "                      first_n_words=10,#  It indicates how many words to print\n",
        "                      figsize=(12, 10),\n",
        "                      fontsize=32,\n",
        "                      plot_wordcloud = False, # Switch to True if you want to plot wordclouds\n",
        "                      lst_clust = [], # Insert a list of integer to get info about a selected number of cluster. It shows info of all the clustrs if empty list\n",
        "                      title_in_plot = \"Clust_\")"
      ],
      "metadata": {
        "id": "40mECfJP4TK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Statistics for Lexical Keyness\n"
      ],
      "metadata": {
        "id": "6Thb9kfIO6mH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A complementary analysis of the sum of Tf-IDF weights is <font color='f28c00'>keyness</font>.  This analysis gives the statistical significance of a keyword's frequency, in a given corpus, relative to a reference corpus.\n",
        "\n",
        "In our case, we compare all the words of the documents of a cluster against the other docuemnts of the corpus. The Log-likelihood Ratio return the lexical specificity of the word for the target cluster. \n",
        "\n",
        "Keyness is generally sensitive to low frequency. For this reason we can also sort Log-likelihood Ratio after having removed words with low frequency.\n",
        "\n",
        "A short description of the method is accessible [here](https://alvinntnu.github.io/NTNU_ENC2036_LECTURES/keyword-analysis.html#statistics-for-keyness)."
      ],
      "metadata": {
        "id": "QdOjyAwQN1bW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_keyness = lexical_keyness(DTM = freq_term_DTM, # DTM\n",
        "                             cls_kmeans = result_kmeans, # Fill with the result of a kmeans clustering\n",
        "                             vocabulary_vectorize = vectorized.vocabulary_, # \n",
        "                             n_cluster = 13) # Number of cluster for which look at the keyness of words\n",
        "# Belowe we choose a frequency threshold to reduce words having low frequency\n",
        "frequency_threshold = 10 # Insert your frequency threshold as integer\n",
        "df_keyness[df_keyness['tot'] > frequency_threshold]['Log-likelihood Ratio'].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "FeoqohmFdSIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Cross-Analysis of the cluster results with the metadata\n",
        "\n",
        "We can use <font color = 'E3A440'>metadata</font> to explore clusters created by the Kmeans. \n",
        "\n",
        "In this subsection, we analyze the `overall_score` and the `sector` of the companies for each cluster. "
      ],
      "metadata": {
        "id": "PQgWZQPvzdBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the chunk below, we prepare data for analysis, by assigning clustering labels to each row in our original dataset. "
      ],
      "metadata": {
        "id": "2ol9xFY9d4VF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column to assign to each company the corresponding cluster\n",
        "df_data_bcorp['cluster_labels'] = result_kmeans.labels_"
      ],
      "metadata": {
        "id": "kTKKy_0x2ExY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The following chunk of code calculates the mean of the `overall_score` of a specific cluster  (i.e, cluster number 10)"
      ],
      "metadata": {
        "id": "V8o9pdVL08X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We compute the .mean() of the `overall_score`for cluster 10\n",
        "n_cluster = 10 # choose the number of cluster\n",
        "df_data_bcorp.loc[result_kmeans.labels_== n_cluster, 'overall_score'].mean()"
      ],
      "metadata": {
        "id": "zDTXpUcfzcGZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6cead11-fdac-427c-b455-f3854650bbb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.26842105263158"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The chunk of code below creates a box plot based on the `overall_score` for each cluster"
      ],
      "metadata": {
        "id": "LPsKYbsX1bIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_bcorp.boxplot('overall_score', by='cluster_labels', figsize=(12, 8))"
      ],
      "metadata": {
        "id": "nzR1HpCL11ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Then, we calculate the mean of the `overall_score` for each cluster"
      ],
      "metadata": {
        "id": "hq8fr0_d3UrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Prepare data\n",
        "mean_score_cluster = df_data_bcorp[['cluster_labels','overall_score']].groupby('cluster_labels').mean()\n",
        "df_data_bcorp[['cluster_labels','overall_score']].groupby('cluster_labels')\n",
        "######Setting the parameters of the graph to plot\n",
        "sns.barplot(x=mean_score_cluster.index, y=mean_score_cluster[\"overall_score\"])\n",
        "##Plotting the graph of the two industry sector under exam\n",
        "plt.legend(prop={'size': 9}, title='Cluster')\n",
        "# plt.title('Density Plot for Each Author')\n",
        "plt.xlabel('Clusters', size=12)\n",
        "plt.ylabel('Overall_score', size=12)\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N4ODDj5rmqZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Here, we extract the median of the `overall_score` for each cluster"
      ],
      "metadata": {
        "id": "bkebNwsa2Jwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_bcorp[['cluster_labels','overall_score']].groupby('cluster_labels').median()"
      ],
      "metadata": {
        "id": "m44iuBgk3zTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we get general descriptive statistics with the method `.describe()`."
      ],
      "metadata": {
        "id": "_4Lf3J3DMxKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_bcorp[['cluster_labels','overall_score']].groupby('cluster_labels').describe()"
      ],
      "metadata": {
        "id": "4_QIbub_MsJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Below, we generate a cross table of `sector` and `cluster_labels`. We use a ratio instead of simple frequency. Thus, we get the proportion of the sector for each cluster."
      ],
      "metadata": {
        "id": "bxAlllRBzDDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose metadata to analyse\n",
        "nominal_metadata_to_analyse = 'sector'\n",
        "#\n",
        "df_test = pd.DataFrame(columns=set(df_data_bcorp[nominal_metadata_to_analyse])) \n",
        "for c_t_ in range(result_kmeans.n_clusters):\n",
        "    df_test.loc[c_t_] = df_data_bcorp.loc[result_kmeans.labels_== c_t_, nominal_metadata_to_analyse].value_counts()\n",
        "df_test = df_test.fillna(0)\n",
        "# Compute Ratio\n",
        "for col in df_test.columns:\n",
        "    df_test[col] = df_test[col] / df_test[col].sum() * 100\n",
        "# Round float to 2 decimals\n",
        "df_test = df_test.round(2)\n",
        "df_test"
      ],
      "metadata": {
        "id": "NsnkQBQovLXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Retrieve most representatives documents by cluster\n",
        "\n",
        "In this subsection, we retrieve the <font color = 'E3A440'>most representative documents for each cluster</font>.\n",
        "\n",
        "In the chunk below, we prepare data computing *cosine similarity* between the *centroid*  of each cluster and all documents it contains."
      ],
      "metadata": {
        "id": "NdtRkHbxSZ7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "df_data_bcorp['Cosine_of_related_cluster'] = 0\n",
        "for idx, centroid in enumerate(result_kmeans.cluster_centers_):\n",
        "    idx_cluster = result_kmeans.labels_ == idx\n",
        "    similarity = cosine_similarity(result_kmeans.DTM_[idx_cluster], centroid.reshape(1, -1)).flatten()\n",
        "    df_data_bcorp.loc[idx_cluster, 'Cosine_of_related_cluster'] = similarity "
      ],
      "metadata": {
        "id": "_1RTfSmHSnqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Below, we print most representatives documents for a specific cluster "
      ],
      "metadata": {
        "id": "BPFFNgwh3bdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## choose the cluster to analyze\n",
        "n_clust = 5\n",
        "df_data_bcorp[df_data_bcorp[\"cluster_labels\"] == n_clust].sort_values(by='Cosine_of_related_cluster', ascending= False)[['company_name','Cosine_of_related_cluster',\"text_web_page\"]]"
      ],
      "metadata": {
        "id": "CEp1b5WnZu2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The following function allows printing a limited number of segments extracted from all those documents which are closer to a target cluster. In other terms, it allows to return some **segments** of the most relevant documents for a target cluster. \n",
        "\n",
        "You need to correctly fill all the arguments of the following function. If you're a beginner with Python, ask help to monitors."
      ],
      "metadata": {
        "id": "EFoVPB4tx_94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_KWIC_clusters(data = df_data_bcorp  , # data used for the analysis \n",
        "                    DTM = tfidf_DTM, # tfidf Matrix used for clustering\n",
        "                    cls_kmeans = result_kmeans, # Clustering results\n",
        "                    vocab = vectorized.vocabulary_, # Vocabulary of the vectorization\n",
        "                    k_to_analyze = 5, # Cluster to analyze\n",
        "                    n_words = 3, # Number of words to be used for the Keyword In Context extraction. These will be words with the higher tfidf values for the target cluster\n",
        "                    n_segms_per_docuemnt = 5, # Number of segment to print for each document\n",
        "                    n_documents = 3, # Number of documents closer to the centroid to use for KWIC extraction\n",
        "                    left_windows = 50, # Left window of characters\n",
        "                    right_windows = 50) # Rigth window of characters"
      ],
      "metadata": {
        "id": "VeS7475AuBS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"exercie\"></a>\n",
        "# Section 4: Exercise\n",
        "\n",
        "In this section participants challenge themselves to reproduce the steps made in sections 1, 2 and 3. The exercise is completed in group mode. Each group **will present the final result of their work** in no more than 5 minutes at the end of the workshop. Below, we describe the steps of the exercise.\n",
        "\n",
        "In the <font color = 'E3A440'>first phase (10 minutes maximum)</font>, each group has to choose a sub-corpus of the whole corpus using metadata, such as `sector`, `industry`, `country`, `state` or `city`. You can choose a combination of them, for example, a sub-corpus containing companies of the agriculture sector of Canada. For analysis of textual data, you can choose between two different textual data field : \n",
        "1. `text_web_page`, which contains data collected by WaybackMachine and used for the first part of this workshop \n",
        "2. `description`, which contain a short description of companies, provided by them to BCorp organization.\n",
        "\n",
        "In the <font color = 'E3A440'>second phase (40 minutes)</font>, you will pass through all the analytical steps, from section 4.2 to section 4.8. In those sections, **_we provide you all the necessary chunks of code_** in order to succeed the exercise. In the most of these chunks of code, you only need to fill some empty parts, which are indicated with the three points `...`. Occasionally, you could modify other parts of the code (for intermediate-level participants only).\n",
        "\n",
        "In the <font color = 'E3A440'>third phase (10 minutes)</font>, you have to complete the interpretation of the results and prepare one or two points to show to the class. \n",
        "\n",
        "Below, we show the two textual fields you can choose for the exercise."
      ],
      "metadata": {
        "id": "eXZZgCj2OWB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_bcorp[\"description\", \"text_web_page\"]"
      ],
      "metadata": {
        "id": "8HL46HTdOWB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Select your sub-corpus\n",
        "\n",
        "Look at metadata and, after discussion with members of your group, <font color = 'E3A440'>select the sub-corpus you want to explore</font>. Be sure to have enough companies in your sub-corpus (around 100 at least). \n",
        "\n",
        "Don't be afraid to ask for help to your monitor for this part!! 😀\n",
        "\n",
        "The following chunk give you an example of subsection. ⚡Pay attention the following subsection does not contain enough documents to work.\n",
        "\n",
        "Below, we provide an example of subsection of rows, regarding to the sector. We assign the result of the subsection to a new dataset named `df_data_bcorp_selection`."
      ],
      "metadata": {
        "id": "qIiO9ZT2OeXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_bcorp_selection = df_data_bcorp[df_data_bcorp[\"sector\"]=='service']"
      ],
      "metadata": {
        "id": "0iLd4oahOdgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Morphological Analysis\n",
        "For the morphological analysis, you can choose between two textual data fields: \n",
        "\n",
        "1. `text_web_page`\n",
        "2. `description`\n",
        "\n",
        "Fill the empty part `...` with the chosen textual column of the subset created above (`df_data_bcorp_selection`)."
      ],
      "metadata": {
        "id": "6Qw6TV8cD6Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_bcorp_selection['text_preprocessed'] = list(nlp.pipe(df_data_bcorp_selection[...], # Fill the empty part\n",
        "                                                             disable = [\"tok2vec\",'parser','ner']))"
      ],
      "metadata": {
        "id": "3XmDQxHaD6Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Filter lexical feature\n",
        "In this section, we load the stopword list for English. You can add words of your choice to the list. This is not necessary to succeed the exercise, but it can help if you want to remove non-meaningful words which appear later in your analysis."
      ],
      "metadata": {
        "id": "7p7L7uHDSqdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download default list of stopword\n",
        "nltk.download('stopwords')\n",
        "# add custom word or words to the stopword list\n",
        "stopwords_list = set(stopwords.words('english') + ['would'])"
      ],
      "metadata": {
        "id": "z6DltLfFSp7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Fill the empty part `...` with the list of POS tag you want to include in your feature selection"
      ],
      "metadata": {
        "id": "bci0ibz3TN52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize empty list\n",
        "text_cleaned = []\n",
        "spacy_lst_object = []\n",
        "# iterate over each preprocessed document\n",
        "for idx, row in df_data_bcorp_selection.iterrows(): \n",
        "    # keep only the lemma for each token that has been tagged as one of these POS tags [\"ADV\", \"ADJ\", \"NOUN\", \"VERB\"]\n",
        "    text = [w.lemma_.lower() for w in row['text_preprocessed'] if w.pos_ in [...] # Fill the empty part here\n",
        "            # AND its lemma IS NOT contained in the stopwords_list AND its lemma has more then 1 character\n",
        "            and w.lemma_.lower() not in stopwords_list and len(w.lemma_.lower())> 1]\n",
        "    text_cleaned.append(text)\n",
        "#\n",
        "df_data_bcorp_selection[\"text_cleaned\"] = text_cleaned"
      ],
      "metadata": {
        "id": "CicbNrbHTM9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Vectorization of lexical features\n",
        "For this step, you have to choose the values for `min_df` and `max_df` arguments. Fill the empty part `...`. At the end of the next chunk of code, we generate the frequency matrix and the Tf-IDF matrix. They are named as follows: \n",
        "1. `freq_term_DTM_selection`\n",
        "2. `tfidf_DTM_selection`\n",
        "\n",
        "The dimensions of the `freq_term_DTM_selection` is printed in order to give you an idea of the number of terms you are keeping in your matrix.\n",
        "\n",
        "Fill the empty parts `...`."
      ],
      "metadata": {
        "id": "BYP-Iiz1T2KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = ..., # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = ..., # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords_list, # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps\n",
        "#\n",
        "freq_term_DTM_selection = vectorized.fit_transform(df_data_bcorp_selection[\"text_cleaned\"])\n",
        "tfidf = TfidfTransformer(norm='l1')\n",
        "tfidf_DTM_selection = tfidf.fit_transform(freq_term_DTM_selection)\n",
        "#\n",
        "freq_term_DTM_selection"
      ],
      "metadata": {
        "id": "gePRWwGZT1kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Descriptive statistics"
      ],
      "metadata": {
        "id": "0-XJ5piIV3l4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the wordcloud of most frequent words your sub-corpus. Fill the empty parts `...` with the frequency matrix you generated in the previous chunk."
      ],
      "metadata": {
        "id": "2lROewv0_sXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vectorized.vocabulary_\n",
        "data_WC = prepare_data_for_WC(..., # Fill with the frequency matrix\n",
        "                              vocab) \n",
        "# Create and generate a word cloud image:\n",
        "wordcloud = WordCloud().generate_from_frequencies(data_WC)\n",
        "# Display the generated image:\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ckfyj7D5Vufi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6 Document clustering\n",
        "\n",
        "Here, we execute the clustering algorithm. Fill the empty parts `...`. We generate the variable `kmeans_result_selection` containing the result of the clustering.\n",
        "\n",
        "Suggestion: choose a number of clusters to generate (`n_cluster`), look at the wordclouds, come back here again to generate a new *kmeans* with another number of clusters, and look again at the result. Get the best partition looking at the wordcloud generated at each execution of the *kmeans*."
      ],
      "metadata": {
        "id": "K5bOvCxhQuVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans_result_selection = Clustering_kmeans(DTM = ..., # Fill with the tfidf matrix\n",
        "                               reduce_lsa = True,\n",
        "                               n_comp_lsa= 300,\n",
        "                               n_clusters = ..., # choose the number of clusters you want to generate\n",
        "                               random_state = 8426,\n",
        "                               max_iter = 1000,\n",
        "                               n_init = 100,\n",
        "                               tol = 0.0001,\n",
        "                               init = \"k-means++\")"
      ],
      "metadata": {
        "id": "oGEWQP7AQt9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.7 Wordcloud by cluster\n",
        "\n",
        "We analyze the most important words for each cluster generate above. We use the Tf-IDF matrix for this operation.\n",
        "Fill the empty parts `...`"
      ],
      "metadata": {
        "id": "NPiNA_rlQ1fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud_par_cluster(WordCloud(), # \n",
        "                      ..., # Fill with the tfidf matrix of your sub-corpus\n",
        "                      ..., # Fill with the results of the clustering\n",
        "                      vectorized.vocabulary_, # \n",
        "                      first_n_words = ...,\n",
        "                      figsize = (12, 10),\n",
        "                      fontsize = 32,\n",
        "                      plot_wordcloud = False, #\n",
        "                      lst_clust = [], #\n",
        "                      title_in_plot = \"Clust_\") #"
      ],
      "metadata": {
        "id": "zulRsZp_Q1fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.8 Read most relevant document by cluster\n",
        "\n",
        "Get the most representative segments of each cluster. \n",
        "Fill the empty parts `...`.\n"
      ],
      "metadata": {
        "id": "acDWHwdARwq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_KWIC_clusters(data = ...  , # data used for analysis \n",
        "                    DTM = ..., # tfidf Matrix used for clustering\n",
        "                    cls_kmeans = ..., # Clustering results\n",
        "                    vocab = vectorized.vocabulary_, # Vocabulary of the vectorization\n",
        "                    k_to_analyze = ..., # Cluster to analyze\n",
        "                    n_words = 3, # Number of words to be used for the Keyword In Context extraction. These will be words with the higher tfidf values for the target cluster\n",
        "                    n_segms_per_docuemnt = ..., # Number of segemtn to print for each document\n",
        "                    n_documents = 3, # Number of documents closer to centroid to use for KWIC extraction\n",
        "                    left_windows = 50, # Left window of character\n",
        "                    right_windows = 50) # Rigth window of character"
      ],
      "metadata": {
        "id": "rRja-PvxR90W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.9 Complete the exercise\n",
        "\n",
        "Prepare your presentation for plenary session. You have 5 minutes to present one or two elements of your group's work. "
      ],
      "metadata": {
        "id": "hfOf7_iLR-MW"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "2022-P4IE-preconference-workshop.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}